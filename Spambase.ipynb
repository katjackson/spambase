{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Spam Email Bayesian Classifier\n",
    "In this exercise I will use sci-kit learn to create and train a Bayesian Classifier to discern spam from other emails. Utilizes the spambase data from UCI Machine Learning Repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I read in the data using pandas and gave it a long list of column headings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>SPAM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...   char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...          0.00        0.000   \n",
       "1             0.00            0.94  ...          0.00        0.132   \n",
       "2             0.64            0.25  ...          0.01        0.143   \n",
       "3             0.31            0.63  ...          0.00        0.137   \n",
       "4             0.31            0.63  ...          0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  SPAM  \n",
       "0                       278     1  \n",
       "1                      1028     1  \n",
       "2                      2259     1  \n",
       "3                       191     1  \n",
       "4                       191     1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columnsx = ['word_freq_make',\n",
    "            'word_freq_address',\n",
    "            'word_freq_all', \n",
    "            'word_freq_3d',\n",
    "            'word_freq_our',\n",
    "            'word_freq_over',\n",
    "            'word_freq_remove',\n",
    "            'word_freq_internet',\n",
    "            'word_freq_order',\n",
    "            'word_freq_mail',\n",
    "            'word_freq_receive',\n",
    "            'word_freq_will',\n",
    "            'word_freq_people',\n",
    "            'word_freq_report',\n",
    "            'word_freq_addresses',\n",
    "            'word_freq_free',\n",
    "            'word_freq_business',\n",
    "            'word_freq_email',\n",
    "            'word_freq_you',\n",
    "            'word_freq_credit',\n",
    "            'word_freq_your',\n",
    "            'word_freq_font',\n",
    "            'word_freq_000',\n",
    "            'word_freq_money',\n",
    "            'word_freq_hp',\n",
    "            'word_freq_hpl',\n",
    "            'word_freq_george',\n",
    "            'word_freq_650',\n",
    "            'word_freq_lab',\n",
    "            'word_freq_labs',\n",
    "            'word_freq_telnet',\n",
    "            'word_freq_857',\n",
    "            'word_freq_data',\n",
    "            'word_freq_415',\n",
    "            'word_freq_85',\n",
    "            'word_freq_technology',\n",
    "            'word_freq_1999',\n",
    "            'word_freq_parts',\n",
    "            'word_freq_pm',\n",
    "            'word_freq_direct',\n",
    "            'word_freq_cs',\n",
    "            'word_freq_meeting',\n",
    "            'word_freq_original',\n",
    "            'word_freq_project',\n",
    "            'word_freq_re',\n",
    "            'word_freq_edu',\n",
    "            'word_freq_table',\n",
    "            'word_freq_conference',\n",
    "            'char_freq_;',\n",
    "            'char_freq_(',\n",
    "            'char_freq_[',\n",
    "            'char_freq_!',\n",
    "            'char_freq_$',\n",
    "            'char_freq_#',\n",
    "            'capital_run_length_average',\n",
    "            'capital_run_length_longest',\n",
    "            'capital_run_length_total',\n",
    "            'SPAM'\n",
    "]\n",
    "spambase = pd.read_csv('spambase.data', header=None, names=columnsx)\n",
    "spambase.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sci-kit learn's train_test_split method, I quickly and easily split each data frame into 60% training data and 40% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(spambase, test_size=0.4, train_size=0.6, random_state=60)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "spambot = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambot.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.78435632808256384"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79021739130434787"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spambot.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find a better model\n",
    "##### different split percentage does not have a significant effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(spambase, test_size=0.2, train_size=0.8, random_state=60)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spammy = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76221498371335505"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spammy.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Out of three predictions based on character, word, or capital frequency alone, the Bayesian model based on word frequency performed the best, even better than the model of all of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2760, 6) (2760,) (1841, 6) (1841,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.77403585008147746"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['char_freq_;',\n",
    "            'char_freq_(',\n",
    "            'char_freq_[',\n",
    "            'char_freq_!',\n",
    "            'char_freq_$',\n",
    "            'char_freq_#',\n",
    "            'SPAM']\n",
    "\n",
    "train, test = train_test_split(spambase[features], test_size=0.4, train_size=0.6, random_state=60)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "bae = MultinomialNB()\n",
    "bae.fit(X_train, y_train)\n",
    "bae.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2760, 48) (2760,) (1841, 48) (1841,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.86094513851167842"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['word_freq_make',\n",
    "            'word_freq_address',\n",
    "            'word_freq_all', \n",
    "            'word_freq_3d',\n",
    "            'word_freq_our',\n",
    "            'word_freq_over',\n",
    "            'word_freq_remove',\n",
    "            'word_freq_internet',\n",
    "            'word_freq_order',\n",
    "            'word_freq_mail',\n",
    "            'word_freq_receive',\n",
    "            'word_freq_will',\n",
    "            'word_freq_people',\n",
    "            'word_freq_report',\n",
    "            'word_freq_addresses',\n",
    "            'word_freq_free',\n",
    "            'word_freq_business',\n",
    "            'word_freq_email',\n",
    "            'word_freq_you',\n",
    "            'word_freq_credit',\n",
    "            'word_freq_your',\n",
    "            'word_freq_font',\n",
    "            'word_freq_000',\n",
    "            'word_freq_money',\n",
    "            'word_freq_hp',\n",
    "            'word_freq_hpl',\n",
    "            'word_freq_george',\n",
    "            'word_freq_650',\n",
    "            'word_freq_lab',\n",
    "            'word_freq_labs',\n",
    "            'word_freq_telnet',\n",
    "            'word_freq_857',\n",
    "            'word_freq_data',\n",
    "            'word_freq_415',\n",
    "            'word_freq_85',\n",
    "            'word_freq_technology',\n",
    "            'word_freq_1999',\n",
    "            'word_freq_parts',\n",
    "            'word_freq_pm',\n",
    "            'word_freq_direct',\n",
    "            'word_freq_cs',\n",
    "            'word_freq_meeting',\n",
    "            'word_freq_original',\n",
    "            'word_freq_project',\n",
    "            'word_freq_re',\n",
    "            'word_freq_edu',\n",
    "            'word_freq_table',\n",
    "            'word_freq_conference',\n",
    "            'SPAM']\n",
    "\n",
    "train, test = train_test_split(spambase[features], test_size=0.4, train_size=0.6, random_state=60)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "bae = MultinomialNB()\n",
    "bae.fit(X_train, y_train)\n",
    "bae.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2760, 3) (2760,) (1841, 3) (1841,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.53340575774035848"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = ['capital_run_length_average',\n",
    "            'capital_run_length_longest',\n",
    "            'capital_run_length_total',\n",
    "            'SPAM']\n",
    "\n",
    "train, test = train_test_split(spambase[features], test_size=0.4, train_size=0.6, random_state=60)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "bae = MultinomialNB()\n",
    "bae.fit(X_train, y_train)\n",
    "bae.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression time!\n",
    "Linear Regression models do not perform as well as Bayesian models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2760, 57) (2760,) (1841, 57) (1841,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/scipy/linalg/basic.py:884: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  warnings.warn(mesg, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.49252035345961842"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train, test = train_test_split(spambase, test_size=0.4, train_size=0.6, random_state=60)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "\n",
    "regr = LinearRegression()\n",
    "regr.fit(X_train, y_train)\n",
    "regr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selective Feautres\n",
    "The following model is based on all word counts and all character counts as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8853883758826725"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['word_freq_make',\n",
    "        'word_freq_address',\n",
    "        'word_freq_all', \n",
    "        'word_freq_3d',\n",
    "        'word_freq_our',\n",
    "        'word_freq_over',\n",
    "        'word_freq_remove',\n",
    "        'word_freq_internet',\n",
    "        'word_freq_order',\n",
    "        'word_freq_mail',\n",
    "        'word_freq_receive',\n",
    "        'word_freq_will',\n",
    "        'word_freq_people',\n",
    "        'word_freq_report',\n",
    "        'word_freq_addresses',\n",
    "        'word_freq_free',\n",
    "        'word_freq_business',\n",
    "        'word_freq_email',\n",
    "        'word_freq_you',\n",
    "        'word_freq_credit',\n",
    "        'word_freq_your',\n",
    "        'word_freq_font',\n",
    "        'word_freq_000',\n",
    "        'word_freq_money',\n",
    "        'word_freq_hp',\n",
    "        'word_freq_hpl',\n",
    "        'word_freq_george',\n",
    "        'word_freq_650',\n",
    "        'word_freq_lab',\n",
    "        'word_freq_labs',\n",
    "        'word_freq_telnet',\n",
    "        'word_freq_857',\n",
    "        'word_freq_data',\n",
    "        'word_freq_415',\n",
    "        'word_freq_85',\n",
    "        'word_freq_technology',\n",
    "        'word_freq_1999',\n",
    "        'word_freq_parts',\n",
    "        'word_freq_pm',\n",
    "        'word_freq_direct',\n",
    "        'word_freq_cs',\n",
    "        'word_freq_meeting',\n",
    "        'word_freq_original',\n",
    "        'word_freq_project',\n",
    "        'word_freq_re',\n",
    "        'word_freq_edu',\n",
    "        'word_freq_table',\n",
    "        'word_freq_conference',\n",
    "        'char_freq_;',\n",
    "        'char_freq_(',\n",
    "        'char_freq_[',\n",
    "        'char_freq_!',\n",
    "        'char_freq_$',\n",
    "        'char_freq_#',\n",
    "        'SPAM']\n",
    "\n",
    "train, test = train_test_split(spambase[cols], test_size=0.4, train_size=0.6, random_state=900)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)\n",
    "\n",
    "spambot = MultinomialNB()\n",
    "\n",
    "spambot.fit(X_train, y_train)\n",
    "spambot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Stats\n",
    "I split the data bases and collected the means for spam and ham into a separate data frame. I tried to visually pick out the most important factors, disregarding the very specific namd and number instances. This worked out pretty well for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam = spambase[spambase.SPAM == 1]\n",
    "not_spam = spambase[spambase.SPAM == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>not_spam</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_freq_make</th>\n",
       "      <td>0.073479</td>\n",
       "      <td>0.152339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_address</th>\n",
       "      <td>0.244466</td>\n",
       "      <td>0.164650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_all</th>\n",
       "      <td>0.200581</td>\n",
       "      <td>0.403795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_3d</th>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.164672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_freq_our</th>\n",
       "      <td>0.181040</td>\n",
       "      <td>0.513955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   not_spam      spam\n",
       "word_freq_make     0.073479  0.152339\n",
       "word_freq_address  0.244466  0.164650\n",
       "word_freq_all      0.200581  0.403795\n",
       "word_freq_3d       0.000886  0.164672\n",
       "word_freq_our      0.181040  0.513955"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_frame = pd.DataFrame(dict(spam = spam.mean(), not_spam = not_spam.mean()))\n",
    "mean_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88810429114611622"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cols = ['word_freq_remove', 'word_freq_3d', 'word_freq_internet',\n",
    "            'word_freq_free', 'char_freq_!',\n",
    "            'word_freq_edu', 'word_freq_re', 'word_freq_george',\n",
    "            'word_freq_hp', 'word_freq_business', 'word_freq_your',\n",
    "            'word_freq_credit', 'SPAM']\n",
    "train, test = train_test_split(spambase[use_cols], test_size=0.4, train_size=0.6, random_state=900)\n",
    "y_train = train.SPAM\n",
    "y_test = test.SPAM\n",
    "X_train = train.drop('SPAM', 1)\n",
    "X_test = test.drop('SPAM', 1)\n",
    "\n",
    "spambot = MultinomialNB()\n",
    "\n",
    "spambot.fit(X_train, y_train)\n",
    "spambot.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best set of features... \n",
    "The best set of features I found to classify emails is a selection of the most significant word frequency counts and the character frequency of the exclamation point. It performs slightly better than the features of all word counts and all character counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
